#+TITLE: Biomass smoke events database 
#+AUTHOR: ivan hanigan
#+EMAIL: ivan.hanigan@sydney.edu.au
#+LATEX_CLASS: ARTICLE
#+LATEX_CLASS_OPTIONS: [A4PAPER]
#+LATEX: \TABLEOFCONTENTS
-----
NB make for both postgres, dev and master sqlite a user 'visitor@visitor.org', visitor
* TODO-List
** DONE add all prior contributors, and farhad's melbourne, murrays canberra
** DONE add contributer name to references
** DONE add DMJS Bowman as author
** DONE add endnote refs in style, and grant refs
** DONE affiliations and  (geoff UCRH)
Postdoctoral research fellow
University Centre for Rural Health,
Sydney Medical School,
University of Sydney, Sydney, NSW, Australia

And

Data Scientist
Spatial Epidemiology Group,
Centre for Research and Action in Public Health,
Health Research Institute,
University of Canberra, Canberra, ACT, Australia
** DONE C:/Temp is temporary, suggest the user move these before they start.
instruction to go into project folder and double click the w2p.cmd
you can go to events page, at bottom export to csv
** DONE disable the URL for install.R (also this is security risk)
** DONE find better place for contributions than readme
# Contriubitions

- 2016-03-17: Events contributed by Dr Salimi, UTAS, using Satellite-Only protocol and a variation of APHEA imputation procedures.

** DONE fix postgres tables.  Try this with backup tables, create shell then insert data.?  or can this happen without insert record?
** DONE general things found when Murray add a reference
DONE source is compulsory, authors is optional
DONE title compulsory
DONE volume optional
DONE general location means geographical location? optional? remove this.
DONE Desirable. This can be URL, doi or weblink of any kind (and date accessed). Also you might want to add the folder location on your computer and the file name.
DONE optional summary (this is what you think of it)
DONE optional abstract (this is the author thought of it)
DONE spell it out somewhere that you can sort descending by clicking ID (twice)
DONE then the 'biomass smoke events' link is where to go to Add an Event.

Adding events TODO how to add a new place?
DONE - place add an Other option
DONE - then under that is a free text place.

WONTDO min date, start date?
WONTDO max date, end date?
no this is ok

make it clear what is compulsory

DONE final step is to email Ivan. and if not that is fine but the expectation is to cite the data (the github)

** DONE landing DB page dot 4 "to enter new data go and register in top right"  remove email ivan.
this becomes dot 5, dot 4 becomes manuscript flesh out dot 
say step 1 register
step 2 add a reference [at this link]
step 3 add dates [at this link]    
** DONE make changes into the postgres tables
** DONE make changes to the protocols page, bring in sync with manuscript, and the manuscript link is to an old pdf in /static
** DONE make for both postgres, dev and master sqlite a user 'visitor@visitor.org', visitor
** DONE make protocol landing page invite users to select a protocol
a) Johnston, b) Farhad, c) bare minimun and d) suggest their own
bare min is just
step 1 get any reference
step 2 get as much bibliographical as possible e.g. author, title, publisher, year, URL, date Accessed
step 3 go to web2py data entry form 

** DONE new manuscript from Murray
*** email from murray 


---------- Forwarded message ---------
From: Murray.Turner <Murray.Turner@canberra.edu.au>
Date: Tue, Jun 26, 2018 at 4:13 PM
Subject: RE: Biomass smoke events database
To: Ivan.Hanigan <Ivan.Hanigan@canberra.edu.au>
Cc: ivan.hanigan@sydney.edu.au <ivan.hanigan@sydney.edu.au>


Hi Ivan,

 

I’ve finished editing the manuscript using the “Fire” journal template.

 

Copy attached.

 

Can you cast your eye over it and suggest any changes?

 

I’ve highlighted your contact details, which you may/may not wish to update.

 

I will also email you an updated EndNote Library that could be sent with the manuscript as an attachment.

 

Don’t forget, if it is accepted and we do need to pay an article processing fee, I can get the money from the ANDS grant.

 

Cheers,

 

Murray

 

From: Ivan.Hanigan 
Sent: Wednesday, 23 May 2018 9:00 AM
To: Murray.Turner <Murray.Turner@canberra.edu.au>
Cc: ivan.hanigan@sydney.edu.au
Subject: Re: Biomass smoke events database

 

 

Hi Murray,

I hope you are well?

I have changed jobs since last time we spoke and now work full time for Uni Sydney with a visiting fellowship at Uni Canberra (so I still live in ACT, but have to do a lot of travel).

I kept meaning to submit the biomass smoke database paper but felt it just does not really fit the Sci Reports mold very well.

 

I wonder if you have time and are willing to revise the manuscript for the journal "Fire"?

http://www.mdpi.com/journal/fire

It is more suitable for the paper, is open access and has 'free publication for well-prepared manuscripts submitted in 2017.'

http://www.mdpi.com/journal/fire/instructions

We can submit this as a "Data Descriptor and Technical Notes: containing a description of a data set, including methods used for collecting or producing the data, where the data set may be found, and information about its use. Technical notes can include detailed descriptions of equipment operations, field methods, models, and monitoring method’s."

Sorry to burden you but I just don't seem to be able to make the time.

 

Kind regards,

Ivan

 

PS my new contact details:

Ivan Hanigan PhD

Data Scientist (Epidemiology)

University Centre for Rural Health

School of Public Health

University of Sydney

 

Ph: 0428 265 976

Email: ivan.hanigan@sydney.edu.au

 

 

From: Murray.Turner
Sent: Friday, 4 August 2017 12:44 PM
To: Ivan.Hanigan; Fay Johnston; Geoff.Morgan@ucrh.edu.au; david.bowman@utas.edu.au; grant.williamson@utas.edu.au; Farhad Salimi; sarah.henderson@ubc.ca
Subject: FW: Biomass smoke events database

 

Hi Ivan,

 

I have read through the manuscript and made a few suggestions based on the Nature Scientific Reports submission checklist .

 

I have also written the opening paragraph for the Discussion section.  I hope it’s okay – very happy to change it if I’m off track.

 

Regards,

 

Murray

 

From: Ivan.Hanigan 
Sent: Tuesday, 1 August 2017 12:07 PM
To: Murray.Turner; Fay Johnston; Geoff.Morgan@ucrh.edu.au; david.bowman@utas.edu.au; grant.williamson@utas.edu.au; Farhad Salimi; sarah.henderson@ubc.ca
Subject: Biomass smoke events database

 

Hi all,

What do you think about putting this revised manuscript in to Nature Scientific Reports?

 

https://www.nature.com/srep/publish/guidelines

 

Can you please let me know in the next week or so?

Thanks Ivan.

 

 

 

From: Ivan.Hanigan 
Sent: Saturday, 8 July 2017 11:27 AM
To: Fay Johnston; Geoff.Morgan@ucrh.edu.au; david.bowman@utas.edu.au; grant.williamson@utas.edu.au; Murray.Turner; Farhad Salimi; sarah.henderson@ubc.ca
Subject: Fw: Decision on your submission to BMC Research Notes -RESN-D-17-00543

 

 

FYI, rejected due to inability to find reviewers in the field. I think the suggestions are good and easy enough to make changes.

Send me any thoughts of another Journal?

From: em.resn.0.54602a.866dc6ed@editorialmanager.com <em.resn.0.54602a.866dc6ed@editorialmanager.com> on behalf of BMC Research Notes - Editorial Office <em@editorialmanager.com>
Sent: Saturday, 8 July 2017 12:12 AM
To: Ivan.Hanigan
Subject: Decision on your submission to BMC Research Notes -RESN-D-17-00543

 

RESN-D-17-00543
Extensible database of validated biomass smoke events for health research
Ivan Charles Hanigan, PhD; Geoffrey G Morgan,, PhD; Grant J Williamson, PhD; Farhad Salimi, PhD; Sarah B Henderson,, PhD; Murray R Turner; David M. J. S. Bowman, PhD; Fay H Johnston, PhD
BMC Research Notes

Dear Dr Hanigan,

Thank you for considering BMC Research Notes for your manuscript (above). I am sorry to inform you that despite much effort we have been unable to obtain appropriate referees for your manuscript. We are therefore handing it back to you and we are closing your file, so that you may submit it elsewhere. We wish you all the best in finding an alternative venue for your research.

While we regret that we have had to come to this decision, it does seem likely to us that a more specialized journal may have better luck in identifying reviewers.

Below you will find the editorial summary from one of our PhD-level qualified manuscript assessors. While we do not feel able to make a decision on whether your manuscript is publishable based on this report, we hope it will be of use when submitting your manuscript elsewhere.

I wish you every success with your research and hope that you will consider us again in the future.

Best wishes,

Dirk Krüger
BMC Research Notes
https://bmcresnotes.biomedcentral.com/



BMC Research Notes | Home page

bmcresnotes.biomedcentral.com

BMC Research Notes publishes scientifically valid research outputs that cannot be considered as full research or methodology articles. We support the research ...








Reviewer reports:
Reviewer 1: "PEER REVIEWER COMMENTS: No academic peer reviewer available.

EDITORIAL STAFF COMMENTS REGARDING REPORTING STANDARDS: Summary: This paper describes the creation of a database that collects evidence linking historical spikes in air pollution with smoke from vegetation fires. Additionally, the authors describe how the database has been extended to be distributed in an open format that allows the research community to add to the history of these events. Research question: There is a clear research question, and the authors have assessed it adequately. The authors thoroughly describe why the data were obtained and provide citations to support the relevance of this paper's aims. Nonetheless, in the Introduction, the authors should provide more information about where the data (that is included in the database studied) come from. Who enters the data? Who maintains and cleans the database? The authors state that this study describes how the database has been extended to be distributed in an open format; what was the prior format? How wide of
an area did it previously cover, and how available was it previously? Methods: The authors provide thorough information on the protocols used to include information in the database. The authors state that the "Bare Minimum Protocol was developed for this paper", but it is unclear how and why this protocol was created and adapted for use in the current database. They also state that any of the above mentioned protocols can be used, or the database contributor can use his/her own protocol, but it is unclear how consistency in data entry is ensured by allowing variations in contributors' choice of protocol. Results: Overall, the diagram presented in Figure 1 is helpful in visualizing the process of utilizing the database and system. However, it is unclear why the flow of the diagram is at a diagonal slant. The flow and direction of activities described would be clearer in a horizontal or vertical presentation. Additionally, it is unclear why the majority of arrows are blue,
while two arrows are green. Is there significance in the color designation? Conclusions and recommendations: Overall, the authors provide useful information and descriptions of the database used to identify historical spikes in particulate matter concentrations and evaluate whether they are caused by vegetation fire smoke or by other means. Nonetheless, the authors should consider tying the Results and Conclusions back to the citations presented in the Introduction more clearly to illustrate the relevance and need for this paper. Additionally, the authors present a description of the database and its expanded form, but they do not present any discussion of testing this expanded form. Do the authors intend to test it to ensure its effectiveness in capturing necessary data? It is unclear what the future direction or applicability of this paper is, and the authors should make that clearer. In general, this paper seems useful, and with additional information added, it may be
suitable for publication. The language quality is sufficient for further consideration or publication."







If improvements to the English language within your manuscript have been requested we recommend that you address this before submitting to another journal. We recommend that you either get your manuscript reviewed by someone who is fluent in English or, if you would like professional help, you can use any reputable English language editing service. We can recommend our affiliates Nature Research Editing Service (http://bit.ly/NRES_BS) and American Journal Experts (http://bit.ly/AJE_BS) for help with English usage. Please note that use of an editing service is neither a requirement nor a guarantee of publication. Free assistance is available from our English language tutorial (https://www.springer.com/gb/authors-editors/authorandreviewertutorials/writinginenglish) and our Writing resources (http://www.biomedcentral.com/getpublished/writing-resources). These cover common mistakes that occur when writing in English.



Author and Reviewer Tutorials - Writing in English | Springer

www.springer.com

 

 

AJE - Editing, Formatting, Translation and Illustration Services for scientific researchers by academic experts | AJE | American Journal Experts

bit.ly

AJE helps research break through with top-quality author services from experienced academics. We can help prepare your manuscript, figures, posters, and more.

 

Writing resources - BioMed Central

www.biomedcentral.com

Welcome to BioMed Central’s writing resources, a guide on writing and publishing a scientific manuscript. You can use the links to the left or below to find advice ...

 

English Language Editing | Author services from Springer Nature

bit.ly

English Editing from Nature Research ✯ Corrects your language errors ✯ Expert editors in your subject area ✯ All academic text, including research papers
** DONE revise readme

** DONE this needs to pick out a lot of stuff from the manuscript on the R package Repo, I am seperating the R pack (tools) from this repo (data and report/manuscript)
** DONE note that when merging develop with master need to ensure that downloaders use sqlite
** WONTDO make downloaded version not require log in
** DONE remove the develop branch
** TODO the github landing page is source code, gh-pages needs edit or removal
** TODO decommission ANU site, make all references to the github, make landing page that says go to github
** TODO what is procedure for deploy postgres master to sqlite develop and then origi master db?
** TODO procedure to upload locations and to update locations with new ones
** TODO rewrite script get_events_and_timeseries.R (direct to R, not in orgmode)
** TODO snip qc from get events
# NB there is an issue when selecting days above 95% but with NA LFS
#            date pm10pct_lag0 lfs_pm10_lag0
# 7282 2013-12-08   0.76670318             0
# 7283 2013-12-09   0.95228642            NA
# 7284 2013-12-10   0.60460022             0
** TODO sqlite doesn't increment the ref id
## 2020-02-18 there was a difference with Williamson at end of 2013, and this is
## in Horsley too
qc <- extracted[extracted$date >= as.Date("2013-01-01") &
          extracted$date <= as.Date("2013-12-31"),
          c("date", "pm25_lag0", "lfs_event", "pm25pct_lag0")]
qc[qc$pm25_lag0 > 25,]
qc[qc$pm25pct_lag0 >= 0.95,]
## yep so the 21st is not identified but is obviously the big one.
## I will add this
## TODO need to come back to this month and check the other high days,
## there was also a notable prescribed burn overnight on 21/22 october
## written about in wikipedia

## ok now make the plot again
with(extracted, plot(date, pm25_lag0, type = "l", ylim = c(0, 100)))
points(
  extract[extracted$lfs_pm25_lag0 == 1, "date"]
                   ,
  extract[extracted$lfs_pm25_lag0 == 1, "pm25_lag0"]
                   , col = 'red', pch = 16, cex = .7
  )

## Horsley found A total of 184 LFS days (2001-2013)

############################################

qc[qc$contributor== "Dr Ivan Hanigan",]
    id          source               title year credentials credentials_other
701 NA modis rapidfire smoke plume visible 2013 modis smoke                  
702 NA modis rapidfire smoke plume visible 2013 modis smoke                  
           authors volume url summary
701 Satellite Data     NA            
702 Satellite Data     NA            
                                                             abstract
701 Ivan submitted this to append data to those identified by Farhad.
702                             Ivan added to append to Farhad's work


dbGetQuery(ch,"select * from biomass_smoke_reference where abstract like 'Ivan added%'")
dbGetQuery(ch,"select max(id) from biomass_smoke_reference")
799
dbSendQuery(ch, "UPDATE biomass_smoke_reference
   SET id = 800 
 WHERE abstract like 'Ivan added%'")

# can use the same ref for many events (but note that cannot use the rapidfire URL this way)
dbGetQuery(ch,"select * from biomass_smoke_reference where abstract like 'Ivan submitted%'")

dbSendQuery(ch, "delete from biomass_smoke_reference
 WHERE abstract like 'Ivan submitted")

## and now the event is also a problem
dbGetQuery(ch,"select max(id) from biomass_smoke_event")
1467
names(dbGetQuery(ch,"select * from biomass_smoke_event"))
dbSendQuery(ch, "insert into biomass_smoke_event (
 id,                         biomass_smoke_reference_id,
 place,                      
 event_type,                 min_date)
values (1468, 800, 'Sydney East', 'bushfire', '2013-10-21')
")
# woops string to float??
# dbSendQuery(ch, "delete from biomass_smoke_event where id = 1468")
** TODO update the spatial data name for Illawara to Illawarra
dbSendQuery(ch, "UPDATE pollution_stations_combined_final
   SET region = 'Illawarra'
 WHERE region = 'Illawara'")

* sqlite works
** TODO sqlite-code for update to ref table CAREFUL THIS MADE ERROR IN THE 1ST RECORD
#+name:sqlite
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sqlite ####
  library(RSQLite)
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "~/tools/web2py/applications/biomass_smoke_events_db/databases/storage.sqlite")
  dbListTables(con)
  dbSendQuery(con, "create table ref_bu20170607 as select * from biomass_smoke_reference")
  dbSendQuery(con, "drop table biomass_smoke_reference")
  
  qc1 <- dbGetQuery(con , "select * from biomass_smoke_reference")
  nrow(qc1)
  698
  qc2 <- dbGetQuery(con , "select * from ref_bu")
  nrow(qc2)
  dbSendQuery(con, "drop table ref_bu")
  
  paste(names(dbGetQuery(con , "select * from ref_bu")), sep = "", collapse = ", ")
  dbSendQuery(con, "insert into biomass_smoke_reference (id, source, credentials, year, authors, title, volume, url, summary, abstract, protocol_used)
   select id, source, credentials, year, authors, title, volume, url, summary, abstract, protocol_used from  ref_bu20170607")
  
  dbGetQuery(con , "select * from biomass_smoke_event where biomass_smoke_reference_id = 1")
  # whoops I deleted a record 1 that was test, and it deleted events
  dbGetQuery(con , "select * from biomass_smoke_reference where id = 1")
  
  library(swishdbtools)
  ch <- connect2postgres2("ewedb_staging")
  replace <- dbGetQuery(ch , "select * from biomass_smoke_event where biomass_smoke_reference_id = 1")
  replace
  ?dbWriteTable
  dbWriteTable(conn = con, "biomass_smoke_event_replace", replace)
  
  # bah, dates!
  dbGetQuery(con , "delete from biomass_smoke_event where biomass_smoke_reference_id = 1")
  paste(names(replace), sep = "", collapse = ", ")
  dbSendQuery(con, "insert into biomass_smoke_event (
  id, biomass_smoke_reference_id, place, event_type, met_conditions, burn_area_ha
  )
  select
  id, biomass_smoke_reference_id, place, event_type, met_conditions, burn_area_ha
  from biomass_smoke_event_replace")
  
  qc1 <- dbGetQuery(ch , "select * from biomass_smoke_event")
  qc2 <- dbGetQuery(con , "select * from biomass_smoke_event")
  nrow(qc1); nrow(qc2)
  max(qc1$id)
  max(qc2$id)
  
  
  
  # add the col
  # after unsuccess using the other method, just add a col
  #dbSendQuery(con, "create table biomass_smoke_reference  as select * from ref_bu20170607")
  dbSendQuery(con, "ALTER TABLE biomass_smoke_reference ADD COLUMN contributor VARCHAR(512)")
#+end_src
** TODO sqlite-code for update to event tab
#+name:sqlite
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sqlite ####
  library(RSQLite)
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "~/tools/web2py/applications/biomass_smoke_events_db/databases_20170607/storage.sqlite")
  dbListTables(con)
  dbSendQuery(con, "create table event_bu as select * from biomass_smoke_event")
  dbSendQuery(con, "drop table biomass_smoke_event")
  
  qc1 <- dbGetQuery(con , "select * from biomass_smoke_event")
  nrow(qc1)
  str(qc1)
  qc2 <- dbGetQuery(con , "select * from event_bu")
  nrow(qc2)
  1252
  str(qc2)
  dbSendQuery(con, "drop table event_bu")
  
  paste(names(dbGetQuery(con , "select * from event_bu")), sep = "", collapse = ", ")
  #dbGetQuery(con, " select
  #id, biomass_smoke_reference_id, place, event_type, min_date, max_date, burn_area_ha, met_conditions #from
  #event_bu")
  
  dbSendQuery(con, "insert into biomass_smoke_event (
  id, biomass_smoke_reference_id, place, event_type, min_date, max_date, burn_area_ha, met_conditions
  )
   select
  id, biomass_smoke_reference_id, place, event_type, min_date, max_date, burn_area_ha, met_conditions
  from event_bu")
  
  ## dbGetQuery(con , "select * from biomass_smoke_event where biomass_smoke_reference_id = 1")
  ## # whoops I deleted a record 1 that was test, and it deleted events
  ## dbGetQuery(con , "select * from biomass_smoke_reference where id = 1")
  
  ## library(swishdbtools)
  ## ch <- connect2postgres2("ewedb_staging")
  ## replace <- dbGetQuery(ch , "select * from biomass_smoke_event where biomass_smoke_reference_id = 1")
  ## replace
  ## ?dbWriteTable
  ## dbWriteTable(conn = con, "biomass_smoke_event_replace", replace)
  
  ## # bah, dates!
  ## dbGetQuery(con , "delete from biomass_smoke_event where biomass_smoke_reference_id = 1")
  ## paste(names(replace), sep = "", collapse = ", ")
  ## dbSendQuery(con, "insert into biomass_smoke_event (
  ## id, biomass_smoke_reference_id, place, event_type, met_conditions, burn_area_ha
  ## )
  ## select
  ## id, biomass_smoke_reference_id, place, event_type, met_conditions, burn_area_ha
  ## from biomass_smoke_event_replace")
  
  ## qc1 <- dbGetQuery(ch , "select * from biomass_smoke_event")
  ## qc2 <- dbGetQuery(con , "select * from biomass_smoke_event")
  ## nrow(qc1); nrow(qc2)
  ## max(qc1$id)
  ## max(qc2$id)
#+end_src

** TODO fix broken dbpy CAREFUL, THE ADD A RECORD BIT CAN GET THINGS OUT OF WHACK
from
https://groups.google.com/forum/#!topic/web2py/kCcRMFmZKB8

In the database's management tool delete/drop the table;

library(RSQLite)
drv <- dbDriver("SQLite")
con <- dbConnect(drv, dbname = "~/tools/web2py/applications/data_inventory_demo/databases/storage.sqlite")
dbListTables(con)
dbSendQuery(con, "drop table dataset")


Trash the table's .table file in the databases folder;

fi <- dir(pattern = "_dataset.table")
for(f in fi){system(sprintf("rm %s", f))}


In db.py set migrate to migrate='tablename.table'

fake_migrate_all = True, migrate = 'dataset.table'

Save db.py

Return to any app site, such as admin/default/design/appname Go to the database administration, 
hit f5

db.py
fake_migrate_all = False, migrate = True

 go to the webform but DONT  insert a record!

Return to db.py to set migrate to migrate=False

  fake_migrate_all = True, migrate = False)
    
** fixing postgres after mods to sqlite
in pgadmin rename the orig as today date

> library(RSQLite)
> drv <- dbDriver("SQLite") 
> con <- dbConnect(drv, dbname = "~/tools/web2py/applications/biomass_smoke_events_db/databases_20170607/storage.sqlite")
> dbListTables(con)
> qc1 <- dbGetQuery(con , "select * from biomass_smoke_event")
> str(qc1)
qc2 <- dbGetQuery(con , "select * from biomass_smoke_reference")
str(qc2) 


CREATE TABLE public.biomass_smoke_reference
(
  id integer NOT NULL DEFAULT nextval('biomass_smoke_reference_id_seq'::regclass),
  source character varying(512),
  title character varying(512),
  year integer,
  credentials character varying(512),
  credentials_other character varying(512),
  authors character varying(512),
  volume integer,
  url character varying(512),
  summary text,
  abstract text,
  protocol_used text,
  CONSTRAINT biomass_smoke_reference_pk PRIMARY KEY (id)
)
WITH (
  OIDS=FALSE
);
ALTER TABLE public.biomass_smoke_reference
  OWNER TO w2p_user;
GRANT ALL ON TABLE public.biomass_smoke_reference TO w2p_user;
GRANT SELECT ON TABLE public.biomass_smoke_reference TO ivan_hanigan;

CREATE TABLE public.biomass_smoke_event
(
  id integer NOT NULL DEFAULT nextval('biomass_smoke_event_id_seq'::regclass),
  biomass_smoke_reference_id integer,
  place character varying(512),
  place_other character varying(512),
  event_type character varying(512),
  min_date date,
  max_date date,
  burn_area_ha double precision,
  met_conditions text,
  CONSTRAINT biomass_smoke_event_pk PRIMARY KEY (id),
  CONSTRAINT biomass_smoke_event_biomass_smoke_reference_id_fk FOREIGN KEY (biomass_smoke_reference_id)
      REFERENCES public.biomass_smoke_reference (id) MATCH SIMPLE
      ON UPDATE NO ACTION ON DELETE CASCADE
)
WITH (
  OIDS=FALSE
);
ALTER TABLE public.biomass_smoke_event
  OWNER TO w2p_user;
GRANT ALL ON TABLE public.biomass_smoke_event TO w2p_user;
GRANT ALL ON TABLE public.biomass_smoke_event TO ivan_hanigan;

** TODO sqlite-code for murray turner update 
#+name:sqlite
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sqlite ####
  library(RSQLite)
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "/home/ivan_hanigan/Dropbox/projects_environment_general_transfers/Biomass_Smoke_Validated_Events/zipped database/web2py/applications/biomass_smoke_events_db/databases/storage.sqlite")
  dbListTables(con)
  
  qc1 <- dbGetQuery(con , "select max(id) from biomass_smoke_reference")
  qc1
  qc1 <- dbGetQuery(con , "select * from biomass_smoke_reference where id = 796")
  t(qc1)
  
  
  qc1 <- dbGetQuery(con , "select max(biomass_smoke_reference_id) from biomass_smoke_event")
  qc1
  qc1 <- dbGetQuery(con , "select * from biomass_smoke_event where biomass_smoke_reference_id >= 794")
  qc1[!is.na(qc1$biomass_smoke_reference_id),1:2]
  qc1[qc1[,2] == 796,]
  # wrong, done by hand and report
#+end_src

** TODO upload the locations for pollution stations from prior works 
#+begin_src R :session *R* :tangle no :exports none :eval no
  library(RSQLite)
  drv <- dbDriver("SQLite")
  ch <- dbConnect(drv, dbname = "databases/storage.sqlite")
  dbListTables(ch)

  input_orig_data <- F
  if(input_orig_data){
  indat <- foreign::read.dbf("/home/Environment_General/Biomass_Smoke_Validated_Events/biosmoke_pollution/data_provided/pollution_stations_combined_final.dbf", as.is = T)
  str(indat)
  indat$the_geom <- NULL
  #dbSendQuery(ch, "drop table pollution_stations_combined_final")
  dbWriteTable(ch, "pollution_stations_combined_final", indat, row.names = F)
  }

  ## QC
  qc <- dbGetQuery(ch, "select *
  from pollution_stations_combined_final")
  str(qc)

  qc <- dbGetQuery(ch, "
  select * from pollution_stations_combined_final where
          region like 'Sydney%'
  ")
  qc

  qc <- dbGetQuery(ch, "select t1.region as studysite, date, avg(pm25_av) as pm2p5
  from
  (select * from pollution_stations_combined_final where
          region like 'Sydney%') t1
  left join combined_pollutants t2
  on t1.site = t2.site
  group by region, date
  ")
  head(qc)
  summary(qc)
  tail(qc)
  qc$date <- as.Date(qc$date)
  with(qc, plot(date, pm2p5, type = "l", col = 'grey', ylim = c(0,110)))
  with(qc, lines(lowess(pm2p5 ~ date, f = 0.01)))
  segments(as.Date('2009-01-01'), 0, as.Date('2009-01-01'), 200, col = 'red')

#+end_src

* upgrade devel sqlite based on pgis

#+name:sqlite
#+begin_src R :session *R* :tangle no :exports none :eval no
  #### name:sqlite ####
  # out with the old
  library(RSQLite)
  drv <- dbDriver("SQLite")
  con <- dbConnect(drv, dbname = "~/tools/web2py/applications/biomass_smoke_events_db/databases/storage.sqlite")
  dbListTables(con)
  
  #dbSendQuery(con, "drop table temp")
  
  dbSendQuery(con, "delete from biomass_smoke_reference")
  
  
  # in with the new
  library(swishdbtools)
  ch <- connect2postgres2("ewedb_staging")
  datref <- dbGetQuery(ch, "select * from biomass_smoke_reference")
  str(datref)
  datevent <- dbGetQuery(ch, "select * from biomass_smoke_event")
  
  dbWriteTable(con, "temp", datref)
  
  
  paste(names(datref), sep = "", collapse = ", ")
  dbSendQuery(con, "insert into biomass_smoke_reference (
  id, source, title, year, credentials, credentials_other, authors, volume, url, summary, abstract, protocol_used, contributor
  )
   select
  id, source, title, year, credentials, credentials_other, authors, volume, url, summary, abstract, protocol_used, contributor
  from temp")
  
  
  # now events
  qc1 <- dbGetQuery(con , "select * from biomass_smoke_event")
  nrow(qc1)
  str(qc1)
  dbSendQuery(con , "delete from biomass_smoke_event")
  
  dbSendQuery(con, "drop table temp")
  str(datevent)
  datevent$min_date  <- as.character(datevent$min_date)
  datevent$max_date  <- as.character(datevent$max_date)
  
  dbWriteTable(con, "temp", datevent)
  
  
  paste(names(datevent), sep = "", collapse = ", ")
  
  dbSendQuery(con, "insert into biomass_smoke_event (
  id, biomass_smoke_reference_id, place, place_other, event_type, min_date, max_date, burn_area_ha, met_conditions
  )
   select
  id, biomass_smoke_reference_id, place, place_other, event_type, min_date, max_date, burn_area_ha, met_conditions
  from temp")

#+end_src
* manuscript DEPRECATED NOW I FINISHED IN WORD
** go manuscript run-able R
#+begin_src R :session *R* :tangle static/manuscript/go_manuscript.R :exports none :padline no :eval yes 
  setwd("/home/ivan_hanigan/tools/web2py/applications/biomass_smoke_events_db/static/manuscript")
  library(knitr)
  library(knitcitations)
  library(rmarkdown)
  bookdown::render_book("index.Rmd", output_dir = "_book",
                        output_format = bookdown::html_chapters(split_by = "none"))
  file.rename("_main.html", "_book/main.html")
  browseURL("_book/main.html")
  #setwd("../..")
#+end_src

#+RESULTS:
: 0

** schematic
- tex 
- then 
cd ~/tools/web2py/applications/biomass_smoke_events_db/static/manuscript
convert -density 300  biosmoke_system_diagram.pdf biosmoke_system_diagram.png

** headers

*** header-manuscript bookdown
# +HEADERS: :tangle  AirPollutionNeighbourhoodExposures/report/BME_manuscript.Rmd :padline yes
# +BEGIN_SRC markdown
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline no
  ---
  title: "Extensible database of validated biomass smoke events for health research"
  author:
  - name: Ivan C. Hanigan,  University of Canberra and University of Sydney, Australia, (Ivan.Hanigan@canberra.edu.au)
  - name: Fay H. Johnston,  University of Tasmania, (Fay.Johnston@utas.edu.au)
  - name: Geoffery G. Morgan,  University of Sydney, (geoffrey.morgan@sydney.edu.au)
  - name: Grant J. Williamson,  University of Tasmania, (grant.williamson@utas.edu.au)
  - name: Farhad Salimi,  University of Sydney, (Farhad.Salimi@utas.edu.au)
  - name: Sarah B.Henderson,  University of British Columbia, (sarah.henderson@ubc.ca)
  - name: Murray Turner,  University of Canberra, (Murray.Turner@canberra.edu.au)
  - name: David M. J. S. Bowman,  University of Tasmania, (david.bowman@utas.edu.au)
  site: bookdown::bookdown_site
  output: bookdown::gitbook
  csl: components/meemodified.csl
  keywords: "Bushfires, Dust storms"
  date:  "Draft `r format(Sys.time(), '%B %d, %Y')`"  
  bibliography: /home/ivan_hanigan/references/library.bib
  ---
        
#+end_src  
*** abstract
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline no
  
  _Abstract_ (291/300 words)
  
  ,**Objective**: The Biomass Smoke Validated Events  Database is an open and extensible data collection recording historical  spikes in air pollution and validation of whether they were caused by  biomass smoke (e.g. from burning vegetation or forest fires). The project  seeks to enhance the discoverability of this data collection and  provides researchers with tools that allow them to add new data, or to use the existing data to study new statistical associations between pollution spikes and health outcomes around those days.
  
  ,**Background**: Epidemiological studies of the health  effects of biomass smoke events have been hindered by the lack of  available datasets that explicitly list the locations and dates of  pollution events from these sources. Extreme air pollution events may  also be caused by dust storms, fossil fuel induced smog events or  factory fires, and so validation is necessary to ensure the events are  from biomass sources. 
  
  ,**Methods**: Several major urban centers and smaller  regional towns in the Australian states of New South Wales, Western  Australia, and Tasmania were selected as they are intermittently  affected by extreme episodes of biomass smoke. Air pollution  data was collated and missing values were imputed. Extreme values were  identified and a range of sources of reference information were assessed  for each date. Reference types included online newspaper archives,  government and research agency records, satellite imagery and a Dust  Storms database.
  
  ,**Results**: This dataset contains validated events of  extreme biomass smoke pollution across Australian cities. The authors  have previously demonstrated the utility of this database in analyses of  hospital admissions and mortality data for these locations to quantify  the pollution-related health effects of these events.
  
  ,**Conclusions**: The database was created using open source software and this makes the prospect for future extensions to the  database possible. 
#+end_src  
*** abs snip
The ability for this database to be extended by  other researchers means that new events can be added, and new  information for already identified events can be described. These  methods provide a systematic framework for retrospective identification  of the air quality impacts of biomass smoke. In this paper, we describe  the database and data aquisition methods, as well as analytical  considerations when validating historical events using a range of  reference types.

This is because if other scientists notice an  ommision or error in these data they can offer an amendment. 

We believe  that this will improve the database and benefit the whole biomass smoke  health research community.
*** background, epi context
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
  
  # Background  
  ## Epidemiological studies of outdoor air pollution
  
  
  For decades, researchers have studied the public health impacts of
  ambient outdoor air pollution, particularly from the effects of
  particulate and gaseous pollutants, especially associated with the
  combustion of coal, petroleum and biomass used for cooking (Pope \&
  Dockery 2006). Far fewer studies have examined the effect of
  intermittent smoke from biomass burning, such as that which occurs in
  bushfires, or from woodsmoke trapped by inversion layers during winter
  months as wood is burned for heating [@Naeher2007].
  
  There is a gap in the epidemiological literature of health effects from
  ambient outdoor air pollution relating to smoke from biomass burning
  such as that from bushfires or woodsmoke from heating. Most literature
  available that focuses on biomass smoke health impacts looks at indoor
  pollution from cooking [@Smith1993]. Particles (and perhaps noxious
  gases) in outdoor pollution from biomass smoke might directly influence
  the respiratory system through their inhalation and lodgement in the
  lungs. Particles may then affect the cardiovascular system after their
  entry into the circulatory system from the alveolae. Indirect effects on
  mental health and wellbeing are also plausible.
  
  Epidemiological studies that investigate the relationship between health
  and air pollution exposures have primarily used time-series methods that
  study variations of some health outcomes such as deaths or
  hospitalisations from specific disease groups [@Peng2008a].
  These outcomes are usually monitored by day across whole cities, and
  relationships with atmospheric variables estimated in regression models.
  These typically focus on daily levels of ambient air pollution measured
  by a network of monitoring sites scattered across a city, time matched
  to the health outcomes on the same day or a few days after. In general
  biomass smoke forms only a small part of the mixture of pollutants in
  the air, however when a bushfire or inversion layer event occurs there
  is often a concomitant spike in the pollution levels primarily composed
  of biomass smoke. There is then the ability to study statistical
  associations between these pollution spikes and the health outcomes
  around those days. Anomalous levels of pollution can be arbitrarily
  defined using a threshold such as the 95th percentile and these might be
  assumed to be biomass smoke days, however there are other events that
  might cause such as spike such as dust storms, factory fires or even sea
  salt being driven by certain wind events. There is a need then to
  validate the dates on which events are ascribed in any correlational
  study of pollution spikes and health that claims the high levels are due
  to biomass smoke.
#+end_src  
*** protocols
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
  
  ## General overview of protocols
  
  ### The Johnston Protocol  
  The Johnston Protocol was the first method our team developed for this project and was published as a peer reviewed journal article in 2011 [@Johnston2011a]. This protocol is considered the most conceptually appealing and rigorous method.  In this protocol, for each location the longest available time-series of daily smoke air pollution is acquired.  In our original study there were up to 13 years
  (between 1994 and 2007) of daily air quality data measured as
  Particulate Matter (PM) less than 10 \(\mu\)m (\(PM_{10}\)) or less than 2.5
  \(\mu\)m (\(PM_{2.5}\)) in aerodynamic diameter were examined. Air
  pollution data were provided by government agencies in the states of
  Western Australia, New South Wales, and Tasmania. Daily averages for
  each site were calculated excluding days with less than 75\% of hourly
  measurements. In Sydney and Perth, where data were collected from
  several monitoring stations, the missing daily site-specific PM
  concentrations were imputed using available data from other proximate
  monitoring sites in the network. The daily city-wide PM concentrations
  were then estimated following the protocol of the Air Pollution and
  Health: a European Approach studies [@Atkinson2001]. TODO cite Katsouyanni
  
  First a 'filling-in' procedure was used to improve data completeness. It
  entailed the substitution of the missing daily values with a weighted
  average, using the weights of the missing sites 3-month average
  proportional to the network average. The weights are calculated against
  the valu## e
  s from the rest of the monitoring stations. The pollutant
  measures from all stations providing data were then averaged to provide
  single, city-wide estimates of the daily levels of the pollutants
  
  For each city, all days in which \(PM_{10}\) or \(PM_{2.5}\) exceeded the 95th
  percentile were identified over the entire time series. These extreme
  values were termed 'events'. A range of sources was examined to
  identify the cause of particulate air pollution events, including
  online news archives, Internet searches for other reports,
  government and research agencies, satellite imagery and a Dust Storms
  database. Satellite images were mostly sourced from XXX, but remotely sensed aerosol optical thickness (AOT) data were also examined, to provide further information about days for which the other
  methods did not.
  
  ### The Salimi Protocol
  In 2016 one of us (FS) extended the biomass smoke database for Sydney.  That project developed a refinement of the Johnston Protocol in which only satellite images were used, not review of other reference material.  In the Salimi protocol the air pollution data is processed in the same way.
  
  
  ### The Bare Minimum Protocol
  
  
  In the Bare Minimum Protocol all that is required for an event to be
  validated is any reference that the contributer deems relevant. This
  can be found through any means including opportunistic collection of
  references in an ad hoc fasion.  This method is the least conceptually
  appealing because it results in a collection of events from times and
  places that have had unequal amounts of research effort expended on
  finding evidence, and therefore may contain systematic biases and data
  that are not 'missing at random'.
#+end_src  
*** dev db
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
      
  # The development of this biomass smoke events database
  
  This open and extensible database was developed by the authors to
  identify historical spikes in particulate matter concentrations and to
  evaluate whether they were caused by vegetation fire smoke or by other
  means. A summary of the protocol for developing this database and a
  summary of the data we collated is published already as a descriptive
  paper [@Johnston2011a]. This paper describes how the
  database has been extended to be able to be distributed in an open,
  extensible format that allows the research community to add to the
  history of these events.
  
  ## System design
  
  ```{r, Schematic, fig.cap = "Schematic diagram of the online database and offline processes for extending the database", echo = F}
  include_graphics("biosmoke_system_diagram.png")
  ```
  
  The system is described in Figure \@ref(fig:Schematic). The procedure
  starts with the master copy of the database that is maintained by
  the Data Manager (DM) in our group. The DM extracts a snapshot of the
  database (with a specific version identifier from the Git version
  control system) and makes a 'standalone' version available on Github.
  This standalone version uses web2py so that it is capable of being
  downloaded and run on any operating system used by other computers.
  Contributers may download that version and use it as a local database.

  If following the Johnston Protocol, the
  contributer needs to have daily air pollution data available, and access
  to the required reference materials for validation (e.g. satellite images, newspaper archives, the dust event database). If the user follows the Salimi Protocol they only require daily air pollution and satellite images.  If they are following the Bare Minimum Protocol then they only require the validation reference document.

  The R package is also available on Github, and contains functions that
  may be used to impute any missing data gaps using the procedure
  as per the APHEA2 study protocol [@Katsouyanni1996]. The R package is
  used by the Johnston and Salimi Protocols to compute the quantiles of the new extended time-series of imputed   pollution data, to identify events above the 95th percentile threshold
  that has been set to define 'extreme events'. 

  The contributer uses the
  web2py data entry forms to add the information that is used to meet the
  validation criteria. Once they complete their review of all events they
  notify the DM either with email or by using the Github 'pull request'
  feature. The DM performs Quality Control (QC) checks and then uploads
  the new data to the online database. The procedure then starts again and
  a new version is loaded into the Github repository with descriptions of
  the additional changes that have been incorporated.
#+end_src  
*** data prep
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
  
  # Detailed data preparation and validation methods
  
  ## Step 1: Source air pollution data
  
  Step 1.0 Source air pollution data. Both time series observations and
  spatial data regarding site locations.
  
  Step 1.1. NSW data downloaded from an online data server. Site locations
  (Lat and Long) obtained from website.
  
  Step 1.2. WA data sent on CD from contacts at the WA Government
  Department, these were hourly data as provided. Cleaned so as only days
  with > 75\% of hours are used. Licence puts restricions on
  our right to provide to a third party. Therefore those observed and
  imputed data are not included, only the events.
  
  Step 1.3. Tasmanian data sent via email from contact at the Department,
  these were daily data.
  
  Step 1.4. All data combined and Quality Control checked in the PostGIS
  database.
  
  ## Step 2. Define spatial extent for cities
  
  The cities and towns were selected based on the aims of the health study
  to investigate Cardio-respiratory disease and air pollution from biomass
  smoke events. These were Albany, Albury, Armidale, Bathurst, Bunbury,
  Busselton, Geraldton, Gosford-Wyong, Hobart, Illawarra, Launceston,
  Newcastle, Perth, Sydney, Tamworth and Wagga Wagga.
  
  The spatial extent of each city and town was devised by intersecting
  Australian Bureau of Statistics Statistical Local Areas (SLAs) from the
  various Census editions. These boundaries were set so give the best
  possible representation of hospital admissions from the population.
  
  Air pollution monitoring sites were then selected on the basis of their
  proximity to these populations.
  
  ## Step 3. Imputation to fill in gaps in the time-series and calculate a network average
  
  In cities where data were collected from several monitoring stations,
  the missing daily site-specific PM concentrations were imputed using
  available data from other proximate monitoring sites in the network. The
  daily city-wide PM concentrations were then estimated following the
  protocol of the Air Pollution and Health: a European Approach studies
  [@Katsouyanni1996].
  
  Step 3.1. Prepare Data. First it was necessary to find the minimum date
  that the series of continuous observations can be considered to start.
  In the Australian datasets the initial observations could not be used
  because the were sometimes only one day per week, only during a
  particular season or of poor quality due to teething problems with
  equipment and procedures. Then it was necessary to identify missing
  dates. Get a list of the sites to include -- that is with more than 70\%
  observed over the time period (as defined after assessing min and max
  dates of period).
  
  Step 3.2. Loop over each station individually and calculate a daily
  network average of all the other non-missing sites (ie an average of all
  stations except the focal station of that iteration in the loop).
  
  Step 3.3. Calculate three monthly seasonal mean of these non-missing
  stations. Calculate a three-month seasonal mean for MISSING site.
  Estimate missing days at missing sites. The missing value was replaced
  by the mean level of the remaining stations, multiplied by a factor
  equal to the ratio of the seasonal (centred three month) mean for the
  missing station, over the corresponding mean from the stations available
  on that particular day.
  
  Step 3.4. Join all sites for city wide averages and fill any missing
  days at the site-level with average of the days immediately before and
  after the missing days (only when this is below a threshold).
  
  Step 3.5 Take the average of all sites per day for city wide averages.
  
  Step 3.6. Fill any missing days at the city-wide level with the average
  of before and after (if this is less than 5\% of days).
  
  ## Step 4. Validate events and identify the causes
  
  Select any dates with PM10 or PM2.5 greater than 95 percentile.
  Manually validate events using the selected Protocol (or potentially some other approach the user defines). Enter the information for each event into the
  custom built data entry forms. For any events with references for
  multiple types of source, assess the liklihood of any single source
  being the dominant source. Double check any remaining 99th percentile
  dates with no references.
  
  ## Step 5. Insert contributed pollution and validated events, and downstream dissemination
  
  To close the loop the data are then inserted back into the DB.

#+end_src  
*** availability
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
  
  # Availability and requirements
  
  - Project name: BiosmokeValidatedEvents
  - Project home page: https://swish-climate-impact-assessment.github.io/BiosmokeValidatedEvents/
  - Operating system(s): R package is platform independent. Data Entry forms are Web2py.
  - Programming language: R and SQL
  - Recommended: PostgreSQL (PostGIS is desirable)
  - License: CC BY 4.0
  - Any restrictions to use: amendments of errors of ommision or commission are invited but will be vetted before insertion into the master database.
  
  
  ## Availability of supporting data
  
  ### Air pollution data provided
  
  The NSW Air pollution data are available to download from
  http://www.environment.nsw.gov.au/AQMS/search.htm
  
  ### Data derived
  
  The data set supporting the results of this article are available in the
  repository from the website
  https://swish-climate-impact-assessment.github.io/biomass_smoke_events_db
  
  We have applied the license under Creative Commons - Attribution 4.0.
  This allows others to copy, distribute and create derivative works
  provided that they credit the original source.
  
  Users should cite the Johnston 2011 Journal of the Air \& Waste
  Management Association as the validation protocol and the Database
  itself as: TBC

#+end_src  
*** refs
#+begin_src R :session *R* :tangle static/manuscript/index.Rmd :exports none :eval no :padline yes
  
  # References
  
    
#+end_src
